{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from my_tools import *\n",
    "from ranked_structural_connectivity import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient:\n",
    "    def __init__(self, plot_path,assessment_path,baseline_file,files_to_remove,vta_path):\n",
    "        self.plot_path = plot_path\n",
    "        self.assessment_path = assessment_path\n",
    "        self.baseline_file = baseline_file\n",
    "        self.files_to_remove = files_to_remove\n",
    "        self.vta_path = vta_path\n",
    "        \n",
    "    def pickle(self, where_pkl):\n",
    "        pickleFile = open(where_pkl, 'wb')\n",
    "        pickle.dump(self, pickleFile)\n",
    "        pickleFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(where_pkl):\n",
    "    with open(where_pkl, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pat1 = unpickle('Pat1.pkl')\n",
    "parcellation_path_Brodmann = '/media/brainstimmaps/DATA/20xx_Projects/2025_DBSinDepression/03_Data/AtlasCollection/Brodmann/Brodmann_known_default.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.09.2018 has nan LEFT stimulation\n",
      "10.09.2018 has nan ROIGHT stimulation\n",
      "10.09.2018 has nan LEFT stimulation\n",
      "10.09.2018 has nan ROIGHT stimulation\n",
      "14.09.2018 has nan LEFT stimulation\n",
      "14.09.2018 has nan ROIGHT stimulation\n"
     ]
    }
   ],
   "source": [
    "sorted_code_list = np.load(Pat1.plot_path + '/sorted_code_list.npy')\n",
    "sorted_code_list = ['VTA_' + \"{0:0=2d}\".format(i) for i in sorted_code_list]\n",
    "loaded_stimulation_dicts = np.load(Pat1.assessment_path + '/corrected_stimulations.npy', allow_pickle='TRUE')\n",
    "\n",
    "dates = np.load(Pat1.plot_path + '/sorted_dates.npy')\n",
    "df = read_assessments(dates, Pat1.assessment_path, Pat1.baseline_file, Pat1.files_to_remove)\n",
    "all_measures = {'{}'.format(scale): df[df.measure == scale] for scale in list(df.measure.unique())}\n",
    "ranks = all_ranks(all_measures)\n",
    "mode = 'Brodmann' #maybe we will like the Dkt at some point\n",
    "labels = get_labels(parcellation_path_Brodmann, mode)\n",
    "# for side in ['left','right']:\n",
    "bilateral_df = bilateral_video(labels, Pat1.vta_path, dates, sorted_code_list)\n",
    "total_weight_day = bilateral_df.groupby(['date']).sum().to_dict()['sum_of_weight']\n",
    "bilateral_df['perc'] = bilateral_df.apply(lambda x: (x.sum_of_weight / total_weight_day[x.date]) * 100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 8\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 8\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 8\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 8\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2271335316746408,\n",
       " 0.2271335316746408,\n",
       " 1.0821226884370032,\n",
       " 2.2213797064889995,\n",
       " 2.2213797064889995,\n",
       " 2.5132726722739274,\n",
       " 2.5132726722739274,\n",
       " 2.71170775982287,\n",
       " 2.71170775982287,\n",
       " 2.908644736599509,\n",
       " 2.973367661656471,\n",
       " 2.973367661656471,\n",
       " 3.0592645243770162,\n",
       " 3.0592645243770162,\n",
       " 3.0592645243770162,\n",
       " 2.8486434872095385,\n",
       " 2.954933442556466,\n",
       " 2.954933442556466,\n",
       " 3.0880577722189124,\n",
       " 3.0400953816138596,\n",
       " 2.9933731293391475,\n",
       " 2.7681495877068105,\n",
       " 3.0400953816138596,\n",
       " 2.9933731293391475,\n",
       " 2.7681495877068105,\n",
       " 3.0400953816138596,\n",
       " 3.0230983976429022,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145,\n",
       " 3.0093703413462145]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portion_area_16_Pat1 = []\n",
    "for date in bilateral_df.date.unique():\n",
    "    portion = bilateral_df[bilateral_df.date == date].sort_values(by='perc',ascending = False).reset_index()\n",
    "    portion['sumsum'] = portion['perc'].cumsum()\n",
    "    print('number of area with % weight above 2.5 :', portion[portion.perc < 2.5].iloc[0].name)\n",
    "    print('area you need to include to describe 85% of the connections', portion[portion.sumsum > 85].iloc[0].name)\n",
    "    print()\n",
    "    portion_area_16_Pat1.append(portion[portion.area == 'BA16']['perc'].item())\n",
    "\n",
    "portion_area_16_Pat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.09.2019 has nan LEFT stimulation\n",
      "20.09.2019 has nan ROIGHT stimulation\n",
      "20.09.2019 has nan LEFT stimulation\n",
      "20.09.2019 has nan ROIGHT stimulation\n"
     ]
    }
   ],
   "source": [
    "Pat3 = unpickle('Pat3.pkl')\n",
    "sorted_code_list = np.load(Pat3.plot_path + '/sorted_code_list.npy')\n",
    "sorted_code_list = ['VTA_' + \"{0:0=2d}\".format(i) for i in sorted_code_list]\n",
    "loaded_stimulation_dicts = np.load(Pat3.assessment_path + '/corrected_stimulations.npy', allow_pickle='TRUE')\n",
    "\n",
    "dates = np.load(Pat3.plot_path + '/sorted_dates.npy')\n",
    "df = read_assessments(dates, Pat3.assessment_path, Pat3.baseline_file, Pat3.files_to_remove)\n",
    "all_measures = {'{}'.format(scale): df[df.measure == scale] for scale in list(df.measure.unique())}\n",
    "ranks = all_ranks(all_measures)\n",
    "mode = 'Brodmann' #maybe we will like the Dkt at some point\n",
    "labels = get_labels(parcellation_path_Brodmann, mode)\n",
    "# for side in ['left','right']:\n",
    "bilateral_df = bilateral_video(labels, Pat3.vta_path, dates, sorted_code_list)\n",
    "total_weight_day = bilateral_df.groupby(['date']).sum().to_dict()['sum_of_weight']\n",
    "bilateral_df['perc'] = bilateral_df.apply(lambda x: (x.sum_of_weight / total_weight_day[x.date]) * 100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of area with % weight above 2.5 : 10\n",
      "area you need to include to describe 85% of the connections 11\n",
      "\n",
      "22\n",
      "number of area with % weight above 2.5 : 10\n",
      "area you need to include to describe 85% of the connections 11\n",
      "\n",
      "22\n",
      "number of area with % weight above 2.5 : 10\n",
      "area you need to include to describe 85% of the connections 11\n",
      "\n",
      "22\n",
      "number of area with % weight above 2.5 : 10\n",
      "area you need to include to describe 85% of the connections 11\n",
      "\n",
      "22\n",
      "number of area with % weight above 2.5 : 11\n",
      "area you need to include to describe 85% of the connections 11\n",
      "\n",
      "22\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 9\n",
      "\n",
      "20\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 8\n",
      "\n",
      "22\n",
      "number of area with % weight above 2.5 : 10\n",
      "area you need to include to describe 85% of the connections 12\n",
      "\n",
      "21\n",
      "number of area with % weight above 2.5 : 11\n",
      "area you need to include to describe 85% of the connections 12\n",
      "\n",
      "23\n",
      "number of area with % weight above 2.5 : 11\n",
      "area you need to include to describe 85% of the connections 12\n",
      "\n",
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.587282449702821,\n",
       " 0.5978901500476581,\n",
       " 0.5849044505783901,\n",
       " 0.5849044505783901,\n",
       " 0.6427902169370707,\n",
       " 0.48903578067677705,\n",
       " 0.38910059942691433,\n",
       " 0.6800884890497356,\n",
       " 0.7037220216527534,\n",
       " 0.7037220216527534]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portion_area_16_Pat3 = []\n",
    "for date in bilateral_df.date.unique():\n",
    "    portion = bilateral_df[bilateral_df.date == date].sort_values(by='perc',ascending = False).reset_index()\n",
    "    portion['sumsum'] = portion['perc'].cumsum()\n",
    "    print('number of area with % weight above 2.5 :', portion[portion.perc < 2.5].iloc[0].name)\n",
    "    print('area you need to include to describe 85% of the connections', portion[portion.sumsum > 85].iloc[0].name)\n",
    "    print()\n",
    "    portion_area_16_Pat3.append(portion[portion.area == 'BA16']['perc'].item())\n",
    "    print(portion[portion.area == 'BA16'].iloc[0].name)\n",
    "portion_area_16_Pat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.03.2020 has nan LEFT stimulation\n",
      "17.03.2020 has nan ROIGHT stimulation\n",
      "06.09.2020 has nan LEFT stimulation\n",
      "06.09.2020 has nan ROIGHT stimulation\n"
     ]
    }
   ],
   "source": [
    "Pat2 = unpickle('Pat2.pkl')\n",
    "sorted_code_list = np.load(Pat2.plot_path + '/sorted_code_list.npy')\n",
    "sorted_code_list = ['VTA_' + \"{0:0=2d}\".format(i) for i in sorted_code_list]\n",
    "loaded_stimulation_dicts = np.load(Pat2.assessment_path + '/corrected_stimulations.npy', allow_pickle='TRUE')\n",
    "\n",
    "dates = np.load(Pat2.plot_path + '/sorted_dates.npy')\n",
    "df = read_assessments(dates, Pat2.assessment_path, Pat2.baseline_file, Pat2.files_to_remove)\n",
    "all_measures = {'{}'.format(scale): df[df.measure == scale] for scale in list(df.measure.unique())}\n",
    "ranks = all_ranks(all_measures)\n",
    "mode = 'Brodmann' #maybe we will like the Dkt at some point\n",
    "labels = get_labels(parcellation_path_Brodmann, mode)\n",
    "# for side in ['left','right']:\n",
    "bilateral_df = bilateral_video(labels, Pat2.vta_path, dates, sorted_code_list)\n",
    "total_weight_day = bilateral_df.groupby(['date']).sum().to_dict()['sum_of_weight']\n",
    "bilateral_df['perc'] = bilateral_df.apply(lambda x: (x.sum_of_weight / total_weight_day[x.date]) * 100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of area with % weight above 2.5 : 10\n",
      "area you need to include to describe 85% of the connections 17\n",
      "\n",
      "8\n",
      "number of area with % weight above 2.5 : 9\n",
      "area you need to include to describe 85% of the connections 13\n",
      "\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.9592323254663806, 7.887129309402314]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portion_area_16_Pat2 = []\n",
    "for date in bilateral_df.date.unique():\n",
    "    portion = bilateral_df[bilateral_df.date == date].sort_values(by='perc',ascending = False).reset_index()\n",
    "    portion['sumsum'] = portion['perc'].cumsum()\n",
    "    print('number of area with % weight above 2.5 :', portion[portion.perc < 2.5].iloc[0].name)\n",
    "    print('area you need to include to describe 85% of the connections', portion[portion.sumsum > 85].iloc[0].name)\n",
    "    print()\n",
    "    portion_area_16_Pat2.append(portion[portion.area == 'BA16']['perc'].item())\n",
    "    print(portion[portion.area == 'BA16'].iloc[0].name)\n",
    "portion_area_16_Pat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99 2.7 0.23 3.09\n"
     ]
    }
   ],
   "source": [
    "portion_area_16_Pat1 = np.asarray(portion_area_16_Pat1)\n",
    "print(np.round(np.median(portion_area_16_Pat1),2), np.round(np.mean(portion_area_16_Pat1),2),np.round(np.min(portion_area_16_Pat1),2),np.round(np.max(portion_area_16_Pat1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59 0.6 0.39 0.7\n"
     ]
    }
   ],
   "source": [
    "portion_area_16_Pat3 = np.asarray(portion_area_16_Pat3)\n",
    "print(np.round(np.median(portion_area_16_Pat3),2), np.round(np.mean(portion_area_16_Pat3),2),np.round(np.min(portion_area_16_Pat3),2),np.round(np.max(portion_area_16_Pat3),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.42 5.42 2.96 7.89\n"
     ]
    }
   ],
   "source": [
    "portion_area_16_Pat2 = np.asarray(portion_area_16_Pat2)\n",
    "print(np.round(np.median(portion_area_16_Pat2),2), np.round(np.mean(portion_area_16_Pat2),2),np.round(np.min(portion_area_16_Pat2),2),np.round(np.max(portion_area_16_Pat2),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.95923233, 7.88712931])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portion_area_16_Pat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
